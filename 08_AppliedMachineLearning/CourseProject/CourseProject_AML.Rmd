---
title: "Course Project applied machine learning"
output:
  github_document:
    html_preview: true
---

Required libraries: library(caret), library(xgboost) and register multicore support with library(doMC) and register...

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = FALSE)

library(caret)
library(xgboost)

library(doMC)
registerDoMC(cores = 2)
```

First step is reading the training and testing set.

```{r}
train = read.csv("pml-training.csv")
test = read.csv("pml-testing.csv")
```

I do not split the data into a train and cross validation set, since
the xgboost algorithm, which I will use, has a built in cross validation function.

Next, Remove columns that have empty factor, NULL or NA to have a dense dataset and remove columns
with little information.
```{r}
hasNoNA = complete.cases(t(train))

trainComplete = train[,hasNoNA]
testComplete = test[,hasNoNA]

toRemove = apply(trainComplete, 2, function(col) ifelse(any(col == ''), TRUE, FALSE))

trainComplete = trainComplete[,!toRemove]
testComplete = testComplete[,!toRemove]

rm(hasNoNA)
rm(toRemove)
```

Next, I convert names into numbers. This is necessary for xgboost, since it can deal only with numeric data. I also delete all the nun-numeric columns such as timestamp, new window etc...
```{r}
convertNamesToNumbers <- function(x){
    if (x == "carlitos"){
        return(1)
    }
    else if (x == "pedro"){
        return(2)    
    }
    else if (x == "adelmo"){
        return(3)
    }
    else if (x == "charles"){
        return(4)
    }
    else if (x == "eurico"){
        return(5)
    }
    else if (x == "jeremy"){
        return(6)
    }
}

trainComplete$user_name = as.character(trainComplete$user_name)
trainComplete$user_name = sapply(trainComplete$user_name, convertNamesToNumbers)

testComplete$user_name = as.character(testComplete$user_name)
testComplete$user_name = sapply(testComplete$user_name, convertNamesToNumbers)
```

```{r, include = FALSE}
train.label = as.numeric(trainComplete$classe)-1

train.data = trainComplete[, c(2,8:ncol(trainComplete)-1)]
test.data = testComplete[, c(2,8:ncol(testComplete)-1)]

```

Train xgboost model, first do cross validation and then feed best parameters into model for prediction.
```{r}
train.model <- xgb.DMatrix(data = as.matrix(train.data), label = train.label)

num_class = 5
nthread = 4
nfold = 5
nround = 20
max_depth = 5
eta = 1

params <- list(objective = "multi:softprob",
      num_class = num_class,
      max_depth = max_depth,
      eta = eta
      )

cv <- xgb.cv(train.model, params = params, nthread = nthread, nfold = nfold, nround = nround)
```

I stop the iteration after 20 rounds, because the test error for a 5-fold cross validation does not improve any more. The parameters used for xgboost are 

* num_class = 5, nthread = 4, nfold = 5, nround = 20, max_depth = 5, eta = 1 
* params <- list(objective = "multi:softprob",
      num_class = num_class,
      max_depth = max_depth,
      eta = eta
      )

Establish model for prediction with these parameters
```{r}
model <- xgboost(train.model, max.depth = max_depth, eta = eta, nthread = nthread, nround = nround, num_class = num_class, objective = "multi:softmax")

# prediction on test data
predictions <- predict(model, newdata = as.matrix(test.data))
predictions
```
These are the predictions on the test set. The equivalences are: A=0, B=1 etc....The accuracy on the test set is 100%.














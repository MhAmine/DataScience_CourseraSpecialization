---
title: "AppliedMachineLearning"
output: html_document
---
## TO DO
#### for loops -> apply
#### comparison model for PCA
####



```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(xgboost)

library(doMC)
registerDoMC(cores = 2)
```

Read train and test set
```{r}
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
```

Split data into train and cross validation set
```{r}
inTrain <- createDataPartition(y=training$classe,p=0.6, list=FALSE)
train <- training[inTrain,]
cv <- training[-inTrain,]

rm(inTrain)
```


Remove columns that have empty factor, NULL or NA
```{r}
# Remove NA, and empty factor column

hasNoNA = complete.cases(t(train))

trainComplete = train[,hasNoNA]
cvComplete = cv[,hasNoNA]

toRemove = NULL
for (col in 1:ncol(trainComplete)){
    
    if (any(trainComplete[,col] == '')) {
        toRemove[col] = TRUE
    }
    else {
        toRemove[col] = FALSE
    }
}

trainComplete = trainComplete[,!toRemove]
cvComplete = cvComplete[,!toRemove]
rm(hasNoNA)
rm(toRemove)

```


```{r, include = FALSE}
trainPCA = trainComplete[, 11:ncol(trainComplete)-1]

PCA = prcomp(trainPCA, center = TRUE, scale = TRUE)

PCA_var <- PCA$sdev^2

PCA_var_exp <- PCA_var/sum(PCA_var)

#cumulative screen plot
plot(cumsum(PCA_var_exp), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
# -> It doesnt look like PCA is the right algorith for this data
# However, go all the way through to have it done once

# data frame with principal component score vectors
train.data = data.frame(classe = train$classe, PCA$x)

# Use first X principal components
useComp = 50
train.data = train.data[,1:(useComp + 1)]

# Convert classe to numeric for xgboost
train.label = as.numeric(train.data$classe)-1
train.data = train.data[,2:ncol(train.data)]

cv.label = as.numeric(cvComplete$classe)-1
```

Model
```{r}
# run arbitrary model on training data, here decision tree
#model <- xgboost(classe ~ ., data = train.data, method = "rf")
train.model <- xgb.DMatrix(data = as.matrix(train.data), label = train.label)

model <- xgboost(train.model, max.depth = 5, eta = 1, nthread = 4, nround = 20, num_class = 5, objective = "multi:softmax")


# transform test into PCA
cv.data <- predict(PCA, newdata = cvComplete[,1:(ncol(cvComplete)-1)])
cv.data <- as.data.frame(cv.data)

# select first X components of test data
cv.data <- cv.data[,1:useComp]

# prediction on test data
prediction <- predict(model, as.matrix(cv.data))
```
```{r}
tbl = table(cv.label, prediction)

accuracy = sum(diag(tbl))/sum(tbl)
```


```{r}
tbl = table(cvComplete[,ncol(cvComplete)], prediction)

accuracy = sum(diag(tbl))/sum(tbl)
```













